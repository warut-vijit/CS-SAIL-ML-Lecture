{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toward Skynet: Intro to AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Overview\n",
    "There are **many** machine learning algorithms. These can be broken down based on what they try to do, and how they are trained.\n",
    "* Supervision\n",
    "  - Supervised: learns to classify data based on provided examples\n",
    "  - Unsupervised: learns to cluster data without human supervision\n",
    "* Training\n",
    "  - Online: learns by looking at each data point once (like in data mining)\n",
    "  - Active: learns by looking at batches of data multiple times\n",
    "* Models \n",
    "  - Simple learning: assumes that the correct model can be found. This is a good assumption but is often impossible\n",
    "  - Agnostic learning: accepts that using a certain algorithm, the perfect model can't always be created.\n",
    "\n",
    "###Analyzing Algorithms\n",
    "* **Representation**: how well can this problem be represented with this model?\n",
    "* **Optimization**: how much can we reduce the error with more training?\n",
    "* **Generalization**: how well does our model perform on unseen testing data?\n",
    "* Problems: does our model underfit? overfit?\n",
    "  - Underfitting: our model isn't expressive enough to represent how the data interact. Solve this by adding more layers, adding more features, etc.\n",
    "  - Overfitting: our model is **too** expressive. This is a more common problem and is an area of active research. How do we solve this? Nobody knows, but regularization, dropout, and limiting features are good starts.\n",
    "  \n",
    "###Evaluation\n",
    "* How do we see how good our machine learning model is?\n",
    "* We cannot test on training data! Can you see why?\n",
    "* We save a portion of our labeled data as a test set and don't train using it.\n",
    "* **Cross-validation** is where we split up the labeled data into many sets (often 5 or 10) and train using all but one of them, and test using the last one. This is often more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Decision Trees\n",
    "\n",
    "####Motivation\n",
    "Given a set of examples, decide how to label the examples into classes. Decision trees try to model human decision processes by looking at one feature at a time. This forms a tree (or flowchart) based on which features improve accuracy the most\n",
    "\n",
    "####Measuring Accuracy\n",
    "We measure how good a decision tree is by looking at its **entropy**, or how noisy the nodes are. This is a weighted sum of how good each node is at classifying examples, multiplied by the relative sizes of the nodes. So, the performance of larger nodes is more important than the performance of smaller ones.\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\sum_{leaf}&space;\\left(\\frac{|leaf|}{|Z|}&space;\\sum_{v}&space;-Pr(v)*log(Pr(v))&space;\\right&space;)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\sum_{leaf}&space;\\left(\\frac{|leaf|}{|Z|}&space;\\sum_{v}&space;-Pr(v)*log(Pr(v))&space;\\right&space;)\" title=\"\\sum_{leaf} \\left(\\frac{|leaf|}{|Z|} \\sum_{v} -Pr(v)*log(Pr(v)) \\right )\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(arr):\n",
    "    \"\"\"Computes the entropy of an array of example tuples\"\"\"\n",
    "    if all(isinstance(elmnt, tuple) for elmnt in arr):\n",
    "        labels = {}\n",
    "        for elmnt in arr:\n",
    "            if elmnt[-1] not in labels:\n",
    "                labels[elmnt[-1]] = 1\n",
    "            else:\n",
    "                labels[elmnt[-1]] += 1\n",
    "        for label in labels:\n",
    "            labels[label] /= float(len(arr))\n",
    "        return sum([-1*labels[index]*log(labels[index],2) for index in labels])\n",
    "    else:\n",
    "        raise ValueError(\"Attempting to calculate entropy of a non-leaf node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Decision Tree Algorithm\n",
    "The decision tree starts out as a single node with all of the examples. Each example has features `(f1, f2, f3, ..., fn)` and a label `v`. The decision tree algorithm is fairly simple compared to many others.\n",
    "1. Consider each feature `f`. Calculate the entropy `H(s)` for each split according to the equation above.\n",
    "2. Split the tree on this feature `f`. All of the examples are moved to their corresponding leaves.\n",
    "3. If each node is homogeneous (labelled the same), stop the algorithm.\n",
    "4. If there are no more features to split on, stop the algorithm.\n",
    "5. For each non-homogeneous node, repeat from step 1. on this new leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selectAttribute(arr, verbose=False):\n",
    "    \"\"\"If arr is not homogeneous, returns index of attribute with greatest information gain\"\"\"\n",
    "    current_entropy = entropy(arr)\n",
    "    entropy_arr = []\n",
    "    for index in xrange(len(arr[0])-1):\n",
    "        sum_entropy = sum([entropy(leaf)*len(leaf)/len(arr) for leaf in split(arr, index).values()])\n",
    "        entropy_arr.append((index, sum_entropy))\n",
    "        if verbose: print(\"Information gain for attribute \"+str(index)+\": \"+str(current_entropy-sum_entropy))\n",
    "    best_att = min(entropy_arr, key=lambda x:x[1])\n",
    "    return best_att[0] if best_att[1] < current_entropy else None  # check to see if no information remains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree(tree, verbose=False):\n",
    "    \"\"\"Recursively perform splits to form decision tree\"\"\"\n",
    "    selected_attribute = selectAttribute(tree, verbose=verbose)\n",
    "    if isHomogeneous(tree) or selected_attribute is None:  # if should not be split any further\n",
    "        labels = {}\n",
    "        for elmnt in tree:\n",
    "            if elmnt[-1] not in labels:\n",
    "                labels[elmnt[-1]] = 1\n",
    "            else:\n",
    "                labels[elmnt[-1]] += 1\n",
    "        label = max([(label,labels[label]) for label in labels], key=lambda x:x[1])[0]\n",
    "        if verbose: print \"\\033[0mCurrent node is: \"+str(tree)+\"\\n\\033[94mNot splitting, label: \"+label+\"\\033[0m\\n\"\n",
    "        return label\n",
    "    else:\n",
    "        best_att = selected_attribute\n",
    "        if verbose: print \"\\033[0mCurrent node is: \"+str(tree)+\"\\n\\033[92mSplitting on: \\033[0m\"+str(best_att)+\"\\n\"\n",
    "        node = {}\n",
    "        children = split(tree, best_att)\n",
    "        for child in children:\n",
    "            node[child] = decision_tree(children[child], verbose)\n",
    "        return (best_att, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(event, tree):\n",
    "    if not isinstance(tree, tuple):\n",
    "        print \"Example \"+str(event)+\" has been labeled as \"+tree\n",
    "        return tree # reached a label\n",
    "    else:\n",
    "        return classify(event, tree[1][event[tree[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Other ML algorithms\n",
    "* Naive Bayes: This is often considered to be the easiest and is often the first ML algorithm that people learn. It is prone to **underfitting** and is not very expressive, but performs surprisingly well\n",
    "<a href=\"https://www.codecogs.com/eqnedit.php?latex=Bayes'&space;Theorem:&space;Pr(x|y)&space;=&space;\\frac{Pr(y|x)*Pr(x)}{Pr(y)}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?Bayes'&space;Theorem:&space;Pr(x|y)&space;=&space;\\frac{Pr(y|x)*Pr(x)}{Pr(y)}\" title=\"Bayes' Theorem: Pr(x|y) = \\frac{Pr(y|x)*Pr(x)}{Pr(y)}\" /></a>\n",
    "* Reinforcement Learning: This is useful for long processes without easily labeled state values, like chess or Atari games. The agent learns by playing or observing for a long period of time and picking up rewards and punishments.\n",
    "* Neural Nets: This is the state of the art but is fairly computationally intensive. They try to model neurons.\n",
    "  - Deep learning: multi-layer neural networks\n",
    "  - Convolutional Neural Networks: good for processing images\n",
    "  - Recurrent Neural Networks: deal with sequences of data\n",
    "  - Generative Adversarial Networks: generate data from labels, eg. generating mugshots from profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}